# Log Stats collection spec 
# Author: havembha@gmail.com,  2021/07/18
# Format
# - LogFile: <file name including path name> - use regular expression syntax to specify a file whose 
#         name may change from one host to another or with time (file name with date/time)
#   - ServiceName: service name that identifies the pattern being searched
#         use this to tie log pattern to many services offered by that process
#         use UpperCase letters to separate words, use letters [a-z|A-Z|0-9] in name
#     PatternPass: string in regular expression syntax to idicate pass instance
#         presence of this string in a line will increment metric <ServiceName>_pass 
#     PatternFail: string in regular expression syntax to idicate pass instance
#         presence of this string in a line will increment metric <ServiceName>_fail
#     PatternCount: string in regular expression syntax to idicate pass instance
#         presence of this string in a line will increment metric <ServiceName>_count
#         use this when Pass or Fail is not needed, just need to account for an incident like alarms
#     Priority: 1 or 2 or 3, default 3
#         This pattern will be searched when currnet average CPU usage is below MaxCPUUsageForEvents[priority]
#
#     PatternSum: leading text (\w+) text (\d+) text (\w+) text (\d+) ....
#                               key1      value1      key2       value2
#         Prometheus metrics ServiceName_keyx_sum
#                value is summed over sampling interval and divided by sampling interval to derive tps
#
#     PatternAverage: leading text (\w+) text (\d+) text (\w+) text (\d+) ....
#                               key1      value1      key2       value2
#         Prometheus metrics ServiceName_keyx_average
#                value is summed over sampling interval and divided by number of samples within that sammpling interval
#
#     PatternDelta: leading text (\w+) text (\d+) text (\w+) text (\d+) ....
#                               key1      value1      key2       value2
#         Prometheus metrics ServiceName_keyx_delta
#                current value is subtracted from previous sample value and summed over the sampling period.
#                at the end of sampling period, tps is calcuated by dividing the sum with sampling period. 
#                When the this script starts, first sample value is stored as previous value, any change from the value seen
#                  in previous run to current run is not computed to keep the tool simple.
#
#     PatternVariablePrefix: extract the regular expression value that matches to the variable prefix definition
#           prefix that to the variable to make unique value
#          serviceName_<variablePrefix>_<patterDelta>_delta
#                                          ^^
#     PatternVariablePrefixGroup: While matching the variable prefix pattern, if the group to be used is not the first one,
#                          use this to specify group number (see example below for more info)
#
#     PatternLabel: extract the regular expression value that matches to the variable PatternLabel,
#                   post all stats extracted in that line with the format
#                   <serviceName>_:<labelName>:_param1=value1,param2=value2,...
#                   These param/value pair will be posted to prometheus gateway with associated label
#                   Web server script posts the data to DB with label like client=<labelName> paramName paramValue 
#     PatternLabelGroup: While matching the label pattern, if the group to be used is not the first one,
#                     use this to specify group number (see example below for more info)
#  
#     SkipGroups: applicable to PatternSum, PatternAverage, PatternDelta type of service definitions
#               where prefix group is used to create dynamic metric variables and post the data to web server
#               Use this to SKIP posting name=value pairs, or any single matching group (groups as matched in regex expression)
#
#     PatternLog: send this log line to web server with jobName=loki so that log line is posted to loki gateway
#                 all log lines of a log file are combined and sent to web server
#                 log lines of different log files are sent with different label value
#                 label name is set to hostname from where log is collected
#                 Unlike all other pattern search, after matching PatternLog, search will continue so that
#                   pattern occurrence will be counted for a log line that is collected
#
#     NOTE - for all patterns above for a given service, 
#               once a search matches, no further search done to match other patterns
#               search for PatternLog continues so that log line collection can still be done after pattern match
#                        within the same service defintion.
#            Across all services, 
#                If log pattern match or stats pattern match for any given service, search stops
#           If search for pattern stats and log are same need to put that under same service name
#           In order to optimize the processing time, it is recommended to put frequently occuring patterns
#            at the beginning of a log file. Less frequently occuring patterns at the end of the list.
#
#     DBDetails: allows sending data to prometheus (default behavior) or to influxdb. For influxdb, can specify
#               separate bucket, and org for each variable. This allows different bucket to be used for different
#                component/platform thus, data retency can be managed separately
#            Web server side configuration will have additional details related to influxdb in JAGlobalVars.yml
#
# NOTE - make no space after pattern strings
#  Escape regular expression special characters in pattern spec like \/
#   Non-special characters match themselves. Special characters don't match themselves −
#
#     \    Escape special char or start a sequence.
#     .    Match any char except newline, see re.DOTALL
#     ^    Match start of the string, see re.MULTILINE
#     $    Match end of the string, see re.MULTILINE
#     [ ]  Enclose a set of matchable chars
#     R|S  Match either regex R or regex S.
#     ()   Create capture group, & indicate precedence
#
#     After '[', enclose a set, the only special chars are −
#      ]    End the set, if not the 1st char
#      -    A range, eg. a-c matches a, b or c
#      ^    Negate the set only if it is the 1st char
#
#     Quantifiers (append '?' for non-greedy) −
#      {m}   Exactly m repetitions
#      {m,n} From m (default 0) to n (default infinity)
#      *     0 or more. Same as {,}
#      +     1 or more. Same as {1,}
#      ?     0 or 1. Same as {,1}
#  For more info on regular expression, refer https://docs.python.org/3/library/re.html
#
#  below section is to execute command on target host, collect stats, log that to log file
#    so that log file processing section can gather application stats and post to web server
#  Execute
#    <CommandSectionName> unique name to specify command information
#    <IntervalInSec> - how often to execute this command
#                     like 60 for every min, 600 for every 10 min
###
### you may use https://regex101.com/ to  test the regular expression definitions on log lines to parse
###
---
GatherLogStatsLogFile: JAGatherLogStats.log
GatherLogStatsCacheFile: JAGatherLogStats.cache
Environment:
   Dev:
     # specify hostname in regular expression 
     HostName: ((...)(d)(...)([0-9][0-9]))|(LAPTOP-QOCNVF0T)
     ### save logs on web server (local cache, in addition to sending to loki)
     SaveLogsOnWebServer: True
     #WebServerURL: https://192.168.1.221:443/cgi-bin/try6.py
     MaxLogTraces: 200
     DataMaskEnabled: False
   Test:
     ## test environment uses the values defined under "All" environment for the variables WebServerURL, DataPostIntervalInSec, DataCollectDurationInSec, DisableWarnings, VerifyCertificate
     HostName: (...)(t)(...)([0-9][0-9])
     ### save logs on web server (local cache, in addition to sending to loki)
     SaveLogsOnWebServer: True
     MaxLogTraces: 200
     DataMaskEnabled: False
   UAT:
     ## UAT environment uses the values defined under "All" environment for the variables DataPostIntervalInSec, DataCollectDurationInSec, DisableWarnings, VerifyCertificate
     HostName: (...)(u)(...)([0-9][0-9])
     WebServerURL: https://192.168.1.221:443/cgi-bin/JASaveStats.py
   Prod1:
     ## prod1 environment uses the values defined under "All" environment for the variables DataPostIntervalInSec, DataCollectDurationInSec, DisableWarnings, VerifyCertificate
     HostName: (...)(p1)(...)([0-9][0-9])
     WebServerURL: https://192.168.1.221:443/cgi-bin/JASaveStats.py
     ### collect max 10 lines per service type within the sampling period
     MaxLogLines: 10
     
   Prod2:
     HostName: (...)(p2)(...)([0-9][0-9])
     WebServerURL: https://192.168.1.221:443/cgi-bin/JASaveStats.py
     ### set to 0 to disable log collection in prod2 environment
     MaxLogLines: 0
     
  ## Keep definition for 'All' environment at the end.
  ##  this is to ensure, Dev, Test, UAT, Prod etc environment specific values are seen/read first and 
  ##  assigned to variables. If a variable is not yet defined under Dev, Test.. like environment,
  ##  and that variable is defined under 'All', value under 'All' will be picked up
  ##  if variable is already defined under environment before, value under 'All' will be ignored.
   All:
     # max time allowed for log file processing during each sampling interval
     #  below 30 second value is half of 60 seconds specified for sampling interval
     MaxProcessingTimeForAllEvents: 15
     # post data to web server per this interval. if sampling interval is 10, post interval is 60,
     #    it will post 6 samples in one post. This is to optimize data post operation.
     DataPostIntervalInSec: 30
     # once the job is started, run until this time. This is to allow job running from crontab at certain periodicity 
     DataCollectDurationInSec: 600
     ### SKIP gathering log stats when average CPU usage % exceeds below limit over previous
     ###   10 DataPostIntervalInSec intervals
     MaxCPUUsageForAllEvents: 80
     ### SKIP gathering log stats with priority 1 when average CPU usage % exceeds below limit
     MaxCPUUsageForPriority1Events: 70
     ### SKIP gathering log stats with priority 2 when average CPU usage % exceeds below limit
     MaxCPUUsageForPriority2Events: 60
     ### SKIP gathering log stats with priority 3 when average CPU usage % exceeds below limit
     MaxCPUUsageForPriority3Events: 50
     ### max log lines per service, per sampling interval
     ###  when log lines exceed this count, from 11th line till last line withing the sampling interval,
     ###    lines will be counted and reported as "..... XXX more lines...."
     ### this is to reduce the data sent to central web server, posted to loki, cached by loki and stored under grafana
     ### set to 0 to disable log collection
     MaxLogLines: 10
     ### trace disabled by default
     MaxLogTraces: 0
     ### datamask enabled by default
     DataMaskEnabled: True
     ### Do not save logs on web server (local cache, in addition to sending to loki)
     SaveLogsOnWebServer: False
     ### while posting data to web server, defaults to False
     DisableWarnings: True
     ### do not verify web server certificate, defaults to True
     VerifyCertificate: False
     # post stats to below web server
     #WebServerURL: https://192.168.1.159:443/cgi-bin/JASaveStats.py
     WebServerURL: https://192.168.1.159:443/JaaduVision/
     ### debug level 0, no debug, 1 to 4, 4 being max details
     DebugLevel: 0
     ### DBDetails - optional, defaults to Prometheus, if present, applies to all service types or measurements 
     ###            where local DBDetails is not specified
     ###     DBType - influxdb, Prometheus
     ###              defaults to Prometheus
     ###     InfluxdbBucket - can be present when DBType is influxdb,  specify different bucket for different application or platform that needs different retency period
     ###              default value is as specified in JAGlobalVars.yml on web server
     ###     InfluxdbOrg - can be present when DBType is influxdb - can specify diff org for diff application or platform
     ###              default value is as specified in JAGlobalVars.yml on web server
     # DBDetails: DBType=Influxdb,InfluxdbBucket=test,InfluxdbOrg=havembha
     ###
     ### Retry sending stats to Web server, 0 to disable retry. Applicable when DBDetails is set to use influxdb.
     ###   if web server is not available, history stats older than this period will be discarded
     ###   
     RetryDurationInHours: 48
     ### add below word at the end of the block start line or single trace line and print translated trace id without '-'
     ###   loki can identify trace id in timestamp line and link to trace record.
     TraceIdPrefix: TraceId=
     # log timestamp format to convert time using time.strptime(). 
     # Refer https://www.tutorialspoint.com/python/time_strptime.htm for details.  
     TimeStampFormat: %Y-%m-%dT%H:%M:%S.%f
     ## id - self-if of a component, used to indicate relationship between various components in a perticular message flow
     ##  this id can be specified in other components definition as parentId if message from this host goes to that host
     TraceId: 1234
     ### if parentId is not defined, it will default to selfId or TraceId
     #TraceParentId:
     # default timestamp format for all logs, can override it later for each log file that has diff format
     PatternTimeStamp: (^\d\d\d\d-\d\d-\d\dT\d\d:\d\d:\d\d\.\d+)

### commands in execute section are executed before processing log files so that
###   any log file created by these execute commands can be processed properly.
Execute:
    Health:
      ### execute below command to gather application stats, output to log file
      Command: tasklist | select-string JATest.py > Health.log
      IntervalInSec: 60
      Priority: 2
#    Uptime:
#      Command: echo "systeminfo | select-string 'Boot Time'" >> Health.log ; systeminfo | select-string "Boot Time" >> Health.log 
#      IntervalInSec: 600
#      Priority: 3

LogFile:
    SyslogError:
      LogFileName: /var/log/syslog
      #LogFileName: /var/log/{HOSTNAME}syslog
      PatternCount: Error
      Priority: 3

    SyslogSession:
      LogFileName: /var/log/syslog
      PatternCount: Started Session
      Priority: 3

    AuthlogInvalidUser:
      LogFileName: /var/log/auth.log
      PatternCount: Invalid user
      Priority: 3

    ApacheGrafana:
      LogFileName: /var/log/apache2/access.log*
      PatternPass: \/grafana(.*)HTTP\/\d.\d" [2..|3..]
      PatternFail: \/grafana(.*)HTTP\/\d.\d" [4..|5..]
      Priority: 2
  
    ApachePrometheus:
      LogFileName: /var/log/apache2/access.log*
      PatternPass: \/prometheus(.*)HTTP\/\d.\d" [2..|3..]
      PatternFail: \/prometheus(.*)HTTP\/\d.\d" [4..|5..]
      Priority: 2

    ApacheSaveStats:
      LogFileName: /var/log/apache2/access.log*
      # 192.168.1.182 - - [30/Jan/2022:15:25:04 +0000] "POST /cgi-bin/JASaveStats.py HTTP/1.1" 200 3343 "-" "curl/7.52.1"
      ### extract the regular expression value that matches to label definition
      PatternLabel: ^(\d+\.\d+\.\d+\.\d+) - -
      ###             ^^^ <-- extract IP address and post the data for this service name with that label
      ### using separate label name for these stats allows aggregation of stats per label name in dashboard
      ###  use 1st group value as variable prefix for the *_avrage metrics variable
      LabelGroup: 1
      ### _pass variable is posted to web server is of the form:
      ###  ApacheSaveStats_:<IPAddress>:_pass=<countPerCollectionInterval>
      ###                   ^^^^^^^^^ <-- this will be separated by web service as label and 
      ###                                  data gets posted to influxb with label client=client1 ApacheSaveStatsPass <count>
      ###                                   This allows further filtering in grafana based on client IP address
      PatternPass: \/JASaveStats.py(.*)HTTP\/\d.\d" [2\d\d|3\d\d]
      PatternFail: \/JASaveStats.py(.*)HTTP\/\d.\d" [4\d\d|5\d\d]
      Priority: 1

    ApacheError:
      LogFileName: /var/log/apache2/error.log* 
      PatternCount: \[cgi:error\] 
      Priority: 1

    TestStats:
      LogFileName: JATest.log*
      ### tps = number of instances divided by sampling interval
      PatternPass: TestMsg Pass
      ### tps = number of instances divided by sampling interval
      PatternFail: TestMsg Fail
      PatternLog: TestMsg Fail
      ### tps = number of instances divided by sampling interval
      PatternCount: TestMsg Count
      # PatternSum: leading text key1 dummy value1 dummy key2 dummy value2 dummy....
      # value is summed over the sampling interval, tps is computed by diving sampling interval
      # line is of the form: 2021-09-18T14:57:07.088995 leading text key1 54 dummy1 key2 27.00 dummy2
      #                                                              kkkk vv        kkkk vvvvv
      ### On grafana, variable name wil be TestStats_key1_sum, TestStats_key2_sum
      PatternSum: leading text (\w+) (\d+) dummy1 (\w+) (\d+.\d+) dummy
      # PatternAverage: leading text key1 dummy value1 dummy key2 dummy value2 dummy....
      # value is summed over the sampling interval, average value is computed by dividing number of samples in that sampling interval
      # line is of the form: 2021-09-18T14:57:07.088995 tps key1 54 dummy1 key2 27.00 dummy2
      #                                                     kkkk  vv       kkkk vvvvv
      # PatternAverage: tps (\w+) (\d+) dummy1 (\w+) (\d+.\d+) dummy  
      # PatternDelta: leading text (\w+) (\d+) dummy1 (\w+) (\d+.\d+) dummy
      # prev sample value is subracted from current sample to find the change or delta value.
      # these delta values are summed over the sampling interval and divided by sampling intervall to get tps
      # line is of the form: 2021-09-18T14:57:07.089993 total key1 9 dummy1 total key2 4.50 dummy2
      #                                                       kkkk v              kkkk vvvv
      ### ### On grafana, variable name wil be TestStats_key1_delta, TestStats_key2_delta
      PatternDelta: tps (\w+) (\d+) dummy1 total (\w+) (\d+.\d+) dummy

    TestStatsWithVarPrefix:
      LogFileName: JATest.log*
      ### extract the regular expression value that matches to the variable prefix definition
      ###   prefix that to the variable to make unique value
      ###   serviceName_<variablePrefix>_<patterDelta>_delta
      PatternVariablePrefix: Stats MicroService(\d) total
      # 2021-10-05T01:09:03.249334 Stats MicroService1 total key1 9 dummy1 total key2 4.50 dummy2
      #                                                      kkk vv              kkk  vvvv
      ### On grafana, variable name wil be TestStatsWithVarPrefix_1_key1_average, TestStatsWithVarPrefix_1_key2_average
      PatternAverage: total (\w+) (\d+) dummy1 total (\w+) (\d+.\d+) dummy2
      Priority: 1

    TestStatsWithVarPrefixGroup:
      LogFileName: JATest.log*
      ### extract the regular expression value that matches to the variable prefix definition
      ###   prefix that to the variable to make unique value
      ###   serviceName_<variablePrefix>_<patterDelta>_delta
      PatternVariablePrefix: Stats MicroService(\d)(\d) total
      ###                                       1    2  <-- group numbers, using index starting from 1
      ###                                           ^^
      ###  use 2nd group value as variable prefix for the *_avrage metrics variable
      VariablePrefixGroup: 2
      # 2021-10-05T01:09:03.249334 Stats MicroService25 total key1 9 dummy1 total key2 4.50 dummy2
      #                                                       kkk vv              kkkk vvvv
      ### On Grafana, variable will be TestStatsWithVarPrefixGroup_5_key1_average, TestStatsWithVarPrefixGroup_5_key2_average
      PatternAverage: total (\w+) (\d+) dummy1 total (\w+) (\d+.\d+) dummy2
      Priority: 1

    TestStatsCSV:
      LogFileName: JATest.log*
      ### NOTE - if regex spec does not have (), it will not be counted as group in regex.
      ###   serviceName_:label:_varName_delta
      PatternLabel: (CSV),(\w+),
      ###              1    2   <-- group numbers, using index starting from 1
      ###                  ^^^
      ###  use 2nd group value as label so that variables can be tracked per client name,
      ###       this allows to graph stats per client in grafana dashboard
      LabelGroup: 2
      ### On Grafana, variable will be TestStatsCSV_key1_delta, TestStatsCSV_key2_delta,... along with client name as label
      ### Stats client2 total key1 39 dummy1 total key2 200.00 key3 19.50 dummy3
      ### 2022-03-26T23:15:05.308415 CSV,client2,39,9.00,19.50
      ###                                        ^^^^^^^^^^^^^^ <-- post these 3 values 
      ###                            ^^^,^^^^^^^ <-- skip posting these as values 
      PatternCSVVariableNames: CSV,clientName,key1,key2,key3
      PatternDelta: (CSV),(\w+),(\d+),(\d+.\d+),(\d+.\d+)
      ###             1    2        3       4      5 <-- group numbers, using index starting from 1
      ###  skip groups 1, and group 2 which is client name, use group 2 for label, post values of groups 3,4,5 to web
      SkipGroups: 1,2
      ### 2022-03-26T23:15:05.308415 CSV,client2,39,9.00,19.50
      ###                             1   2       3   4    5 <-- group numbers
      ### variable and value posted to web server is of the form:
      ###  TestStatsCSV_:client2:key1_delta=39   <-- group 3
      ###                 ^^^^^ <-- this will be separated by web service as label and 
      ###                                   data gets posted to prometheus with label client=client2 varName varValue
      ###                                   This allows further filtering in grafana based on client names
      ###  TestStatsCSV_:client2:key2_delta=<value like 9>    <-- group 4
      ###  TestStatsCSV_:client2:key3_delta=<value like 19.5> <-- group 5
      Priority: 1

    TestStatsWithLabel:
      ### send this data to "test" bucket and "havembha" org in influxdb
      ###   this allows use of separate bucket and org per component/platform, per service type or per variable 
      DBDetails: DBType=Influxdb,InfluxdbBucket=test,InfluxdbOrg=havembha
      LogFileName: JATest.log*
      # 2021-10-31T19:11:12.096877 Stats client1 total key1 34 dummy1 total key2 100.00 key3 100.00 dummy3
      # 2021-10-31T19:11:12.096877 Stats client2 total key1 34 dummy1 total key2 200.00 key3 200.00 dummy3
      #                                  ^^^^^^^ <-- will be posted as client label value to DB
      ### extract the regular expression value that matches to label definition
      PatternLabel: Stats (\w+) total
      ###                  ^^^ <-- extract this and post the data for this service name with that label
      ### using separate label name for these stats allows aggregation of stats per label name in dashboard
      ###  use 1st group value as variable prefix for the *_avrage metrics variable
      LabelGroup: 1
      #2021-10-31T19:11:12.096877 Stats client1 total key1 34 dummy1 total key2 100.00 key3 100.00 dummy3
      #                                               1     2               3    4      5    6     
      #                                                                    ^^^^^^^^ <-- SKIP posting this data
      SkipGroups: 3,4
      PatternAverage: total (\w+) (\d+) dummy1 total (\w+) (\d+.\d+) (\w+) (\d+.\d+) dummy3
      ### variable and value posted to web server is of the form:
      ###  TestStatsWithLabel_:client1:key1_average=34.67
      ###                     ^^^^^^^^^ <-- this will be separated by web service as label and 
      ###                                   data gets posted to influxb with label client=client1 varName varValue
      ###                                   This allows further filtering in grafana based on client names
      Priority: 1

    TestStatsRepeatFieldValueSpec:
      ### send this data to "test" bucket and "havembha" org in influxdb
      ###   this allows use of separate bucket and org per component/platform, per service type or per variable 
      DBDetails: DBType=Influxdb,InfluxdbBucket=test,InfluxdbOrg=havembha
      LogFileName: JATest.log*
      # 2021-11-28T14:00:03.228652 ValuePair client11,key1=39,key2=100.00,key3=19.50
      # 2021-11-28T14:00:03.228829 ValuePair client12,key1=69,key2=200.00,key3=34.50
      ### extract the regular expression value that matches to label definition
      PatternVariablePrefix: ValuePair client(\d)(\d)
      ###                                         ^^
      ###  use 2nd group value as variable prefix for the *_avrage metrics variable
      VariablePrefixGroup: 2
      # 2021-11-28T14:00:03.228652 ValuePair client11,key1=39,key2=100.00,key3=19.50
      #                                               1    2  3    4      5    6     
      #                                                       ^^^^^^^^ <-- SKIP posting this data
      SkipGroups: 3,4
      PatternAverage: (\w+)=(\d+)
      Priority: 1

    # Trace serviceName
    TestTrace:
      # post trace data to zipkin
      DBDetails: DBType=zipkin
      LogFileName: JATest.log*
      # 2022-05-30T15:17:58.938264 TraceId=0000000000000001 Service1 status=202 test trace line 1
      # trace id is hexa decimal number 0-9, a-f, A-F, trace duration at the end of the line
      # regex group:                1                                 2         3          4      5         6      7      8               9
      PatternTimeStamp: (^\d\d\d\d-\d\d-\d\dT\d\d:\d\d:\d\d\.\d+)( TraceId=)([0-9a-fA-F]+)( )(Service1)( status=)(\d+)( test trace line )(\d+)
      #                                                                                                                                  ^^^^ <-- duration
      #                                                                                                           ^^^^  <-- status code
      #                                                                                       ^^^^ <-- label
      #                                                                         ^^^^^^^^^^ <-- trace Id
      #                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <-- timestamp
      # group id where timestamp is present in trace log
      TimeStampGroup: 1
      # log timestamp format to convert time using time.strptime(). 
      # Refer https://www.tutorialspoint.com/python/time_strptime.htm for details.  
      TimeStampFormat: %Y-%m-%dT%H:%M:%S.%f
      # all trace info in single line
      TraceSingleLine: True
      # trace id group in log line
      TraceIdGroup: 3
      # trace name
      TraceLabelGroup: 5
      ## group where status code is present
      TraceStatusGroup: 7
      PatternTraceStatus: (202|404)
      # event duration is at this group id
      DurationGroup: 9
      ### trace duration in milliseconds, multiply by 1000 to concert to microsencds before posting
      DurationMultiplier: 1000
      Priority: 1
      # DebugLevel: 4

    # Trace serviceName
    TestTraceWithSkipGroups:
      # post trace data to zipkin
      DBDetails: DBType=zipkin
      LogFileName: JATest.log*

      # 2022-05-30T15:17:58.938412 TraceId=0000000000000001 Service2 account=1234 Name=JaaduVision test trace line 1
      # trace id is hexa decimal number 0-9, a-f, A-F, trace duration at the end of the line
      # regex group:             1                                      2          
      # regex group:                1                                     2         3         4     5           6     7      8      9   10  11
      PatternTimeStamp: (^\d\d\d\d-\d\d-\d\dT\d\d:\d\d:\d\d\.\d+)( TraceId=)([0-9a-fA-F]+)( )(Service2)( account=)(\d+)( Name=)(\w+)(.*)(\d+)
      #                                                                                                                                     ^^^^ <-- duration
      #                                                                                                             ^^^          ^^^  <-- skip groups
      #                                                                                       ^^^^ <-- label
      #                                                                         ^^^^^^^^^^ <-- trace Id
      #                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <-- timestamp
      # group id where timestamp is present in trace log
      TimeStampGroup: 1
      # log timestamp format to convert time using time.strptime(). 
      # Refer https://www.tutorialspoint.com/python/time_strptime.htm for details.  
      TimeStampFormat: %Y-%m-%dT%H:%M:%S.%f
      # all trace info in single line
      TraceSingleLine: True
      TraceIdGroup: 3
      TraceLabelGroup: 5
      # do not post values of these groups, just mask them. Here, account number and name are MASKED
      SkipGroups: 7,9
      # event duration is at this group id
      DurationGroup: 11
      ### trace duration in milliseconds, multiply by 1000 to concert to microsencds before posting
      DurationMultiplier: 1000
      Priority: 2
      # DebugLevel: 4

    # Trace serviceName
    TestTraceBlock:
      # post trace data to zipkin
      DBDetails: DBType=zipkin
      LogFileName: JATest.log*
      # all trace info in single line
      TraceSingleLine: False
      # log block of the form:
      # 2022-06-10T23:28:33.906468 Block1 Service3 test trace line
      # 2nd line of block status=404
      # 3rd line of block 
      # 4th line of block 00000000000000cb
      # 5th line of block 
      # 6th line of block 
      #                                     1                              2         3       4
      PatternTraceBlockStart: (^\d\d\d\d-\d\d-\d\dT\d\d:\d\d:\d\d\.\d+)( Block1 )(Service3 )(.*)
      #                                                                           ^^^^^^^^ <-- label group
      #                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ <-- timestamp
      # group id where timestamp is present in trace log
      TimeStampGroup: 1
      # commented below so that it will use global definition
      # TimeStampFormat: %Y-%m-%dT%H:%M:%S.%f
      #
      # trace label - trace label can be on it's own line or
      #               can be part of traceId or traceBlockStart line
      TraceLabelGroup: 3

      ### line with status code
      # 2nd line of block status=404
      # regex group:               1                  2
      PatternTraceBlockStatus: (2nd line of block status=)(\d+)
      TraceStatusGroup: 2
      PatternTraceStatus: (202|404)

      # 3rd line of block
      PatternTraceBlockContains: (3rd )(line of block)
      # 4th line of block 00000000000000c
      # regex group:    1    2           3        4
      PatternTraceId: (.*)( block )([0-9a-fA-F]+)$
      #                             ^^^^^^^^^^^^ <--- trace id
      # trace id group in log line
      TraceIdGroup: 3

      # 5th line has SKIP words
      # 5th line of block account=1234 Name=JaaduVision
      # regex groups: 1       2     3       4   5
      PatternSkip: (.*)(account=)(\d+)( Name=)(\w+)(.*)
      SkipGroups: 3,5

      ### Block end line, use at least two groups so that it goes through skip group checks
      PatternTraceBlockEnd: (6th line)( of block)
      ### add below word at the end of the block start line or single trace line and print translated trace id without '-'
      ###   loki can identify trace id in timestamp line and link to trace record.
      TraceIdPrefix: TraceId=
      Priority: 2
      # DebugLevel: 4

